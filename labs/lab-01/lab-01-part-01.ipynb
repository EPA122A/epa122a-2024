{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed1d679",
   "metadata": {},
   "source": [
    "#  EPA-122A *Spatial* Data Science\n",
    "\n",
    "\n",
    "## Lab 1 - part 1: Data, Grammar and Engineering (Pandas)\n",
    "\n",
    "**TU Delft**<br>\n",
    "**Q2 2024**<br>\n",
    "**Instructors:** Giacomo Marangoni, Theodoros Chatzivasileiadis <br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66981b92",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Learning Goals](#section0)\n",
    "* [Data Munging](#section1)\n",
    "    * [Dataset](#section_1_1)\n",
    "    * [Data, Sliced and Diced](#section_1_2)\n",
    "* [Visual Exploration](#section2)\n",
    "    * [Histogram](#section_2_1)\n",
    "    * [Kernel Density Plots](#section_2_2)\n",
    "    * [Line and Bar Plots](#section_2_3)\n",
    "* [(Un)tidy Data](#section3)\n",
    "    * [Grouping, transforming, aggregating](#section_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcad639",
   "metadata": {},
   "source": [
    "## Learning Goals <a class=\"anchor\" id=\"section0\"></a>\n",
    "\n",
    "- Obtain the basic tools to manipulate, investigate and visualise the data.\n",
    "- Understand the concept of tidy/untidy data and how to prepare the data.\n",
    "\n",
    "This notebook covers the basic tools of data analysis that are **needed** for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cf843",
   "metadata": {},
   "source": [
    "## Data Munging <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "Real world datasets are messy. There is no way around it: datasets have \"holes\" (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to [munge](https://dictionary.reference.com/browse/munge) them. As has been correctly pointed out in many outlets ([e.g.](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0)), much of the time [spent](https://twitter.com/BigDataBorat/status/306596352991830016) in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but has to do with much more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\n",
    "\n",
    "For how labor intensive and relevant this aspect is, there is surprisingly very little published on patterns, techniques, and best practices for quick and efficient data cleaning, manipulation, and transformation. In this session, you will use a few real world datasets and learn how to process them into Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the bread and butter of data analysis and scientific computing in Python. These are fundamental tools that are constantly used in almost any task relating to data analysis.\n",
    "\n",
    "This notebook covers the basic and the content that is expected to be learnt by every student. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at. As a companion to this introduction, there is an additional notebook (see link on the website page for Lab 01) that covers how the dataset used here was prepared from raw data downloaded from the internet, and includes some additional exercises you can do if you want dig deeper into the content of this lab.\n",
    "\n",
    "In this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\n",
    "\n",
    "Before we get our hands data-dirty, let us import all the additional libraries we will need, so we can get that out of the way and focus on the task at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e737e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures visualizations are plotted inside the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import os  # This provides several system utilities\n",
    "import pandas as pd  # This is the workhorse of data munging in Python\n",
    "import numpy as np  # This is for general numerical operations\n",
    "import seaborn as sns  # This allows us to efficiently and beautifully plot\n",
    "import os  # This provides several system utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bea231",
   "metadata": {},
   "source": [
    "### Dataset <a class=\"anchor\" id=\"section_1_1\"></a>\n",
    "\n",
    "We will be exploring some of the characteristics of the population in The Hague. To do that, we will use a dataset that contains population counts, split by ethnic origin: https://denhaag.incijfers.nl/jive. Let us first set the path to the file where we store the data we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! You need to specify the path to the data in *your* machine\n",
    "# If you have placed the data folder in the same directory as this notebook,\n",
    "# you would do:\n",
    "\n",
    "f = \"data/DenHaag\"  # Path to file containing the table\n",
    "\n",
    "# Check to see if the path is correct and works. If you have set it\n",
    "# correctly, you should obtain the following list\n",
    "os.listdir(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49464d55",
   "metadata": {},
   "source": [
    "**IMPORTANT**: the path above might look different in your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b81cc",
   "metadata": {},
   "source": [
    "To read a \"comma separated values\" (`.csv`) file, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to work with DenHaag_.csv. Let's adjust the path:\n",
    "f = \"data/DenHaag/DenHaag_.csv\"\n",
    "db = pd.read_csv(f, index_col=\"Buurten\")  # Read the table in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291d11d",
   "metadata": {},
   "source": [
    "Let us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n",
    "\n",
    "* We are using the method `read_csv` from the `pandas` library, which we have imported with the alias `pd`.\n",
    "* In this form, all that is required is to pass the path to the file we want to read, which in this case we have created by concatenating two strings. We can see the full path we have used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed90c93",
   "metadata": {},
   "source": [
    "* the `sep` argument defines the seperator. It is default set to `,` which is also the standard deperator in a csv file. In our case the dataset is seperated by `;` however.\n",
    "* The argument `index_col` is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\n",
    "* We are using `read_csv` because the file we want to read is in the `csv` format. However, `pandas` allows for many more formats to be read (and written, just replace `read` by `to`! For example, `read_csv` reads in, `to_csv` writes out). A full list of formats supported may be found [here](https://pandas.pydata.org/pandas-docs/version/0.18.1/io.html).\n",
    "\n",
    "### Data, Sliced and Diced <a class=\"anchor\" id=\"section_1_2\"></a>\n",
    "\n",
    "Now we are ready to start playing and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the buurtens in The Hague, how many people live in each, by the region of the world where they were born. Now, let us learn a few cool tricks built into `pandas` that work out-of-the box with a table like ours.\n",
    "\n",
    "* Inspecting what it looks like. We can check the top (bottom) X lines of the table by passing X to the method `head` (`tail`). For example, for the top/bottom five lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df833f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8328fda",
   "metadata": {},
   "source": [
    "- Now lets get an overview of data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedaf357",
   "metadata": {},
   "source": [
    "\n",
    "Note how the output is also a `DataFrame` object, so you can do with it the same things you would with the original table (e.g. writing it to a file). In this case, the summary might be better presented if the table is \"transposed\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90445d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09de3ab",
   "metadata": {},
   "source": [
    "* Equally, common descriptive statistics are also available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain minimum values for each table\n",
    "db.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the minimum population share of Marokaans\n",
    "db[\"% Marokkaans|2023\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain standard deviation for the row `117 De Rivieren`,\n",
    "# Does this value have any informative value for us?\n",
    "db.loc[\"117 De Rivieren\", :].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a489e59",
   "metadata": {},
   "source": [
    "* Creation of new variables: we can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503aa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One shot\n",
    "total = db.sum(axis=1)\n",
    "# Print the top of the variable\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing up a subsection of variables:\n",
    "Non_Dutch = (\n",
    "    db[\"% Turks|2023\"]\n",
    "    + db[\"% Marokkaans|2023\"]\n",
    "    + db[\"% Surinaams|2023\"]\n",
    "    + db[\"% Antilliaans|2023\"]\n",
    "    + db[\"% Overig niet-westers|2023\"]\n",
    "    + db[\"% Westers|2023\"]\n",
    ")\n",
    "# Print the top of the variable\n",
    "Non_Dutch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcced017",
   "metadata": {},
   "source": [
    "Note how we are using the command `sum`, just like we did with `max` or `min` before but, in this case, we are not applying it over columns (e.g. the max of each column), but over rows, so we get the total sum of populations by areas.\n",
    "\n",
    "Once we have created the variable, we can make it part of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"% Non-Dutch | 2023\"] = Non_Dutch\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905974aa",
   "metadata": {},
   "source": [
    "* Assigning new values: we can easily generate new variables with scalars, and modify those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New variable with all ones\n",
    "db[\"ones\"] = 1\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca9454",
   "metadata": {},
   "source": [
    "And we can modify specific values too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[\"03 Scheveningen Badplaats\", \"ones\"] = 3\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de7496",
   "metadata": {},
   "source": [
    "* Permanently deleting variables is also within reach of one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fef3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del db[\"ones\"]\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eea33f",
   "metadata": {},
   "source": [
    "* Index-based querying.\n",
    "\n",
    "We have already seen how to subset parts of a `DataFrame` if we know exactly which bits we want. For example, if we want to extract the total population and the share of Turkish people living in the first four neighbourhoods in the table, we use `loc` with lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_share_Turkish_first4 = db.loc[\n",
    "    [\n",
    "        \"01 Oud Scheveningen\",\n",
    "        \"02 Vissershaven\",\n",
    "        \"03 Scheveningen Badplaats\",\n",
    "        \"04 Visserijbuurt\",\n",
    "    ],\n",
    "    [\"Aantal inwoners per 1-1|2023\", \"% Turks|2023\"],\n",
    "]\n",
    "\n",
    "total_share_Turkish_first4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713d5b3",
   "metadata": {},
   "source": [
    "* Querying based on conditions.\n",
    "\n",
    "However, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, `DataFrames` support selection based on conditions. Let us see a few examples. Suppose we want to select...\n",
    "\n",
    "*... areas with more than 2,500 people in Total*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a086d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2500 = db.loc[db[\"Aantal inwoners per 1-1|2023\"] > 2500, :]\n",
    "m2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a73aa",
   "metadata": {},
   "source": [
    "*... areas where there are no more than 10 people aged above 80.*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm10_above_eighty = db.loc[db[\"80 jaar en ouder|2023\"] <= 10, :]\n",
    "nm10_above_eighty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4945da",
   "metadata": {},
   "source": [
    "*... areas with exactly nine person people aged above 80*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_above_eighty = db.loc[db[\"80 jaar en ouder|2023\"] == 9, :]\n",
    "nine_above_eighty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf2523",
   "metadata": {},
   "source": [
    "**Pro-tip**: these queries can grow in sophistication with almost no limits. For example, here is a case where we want to find out the areas where European population is less than half the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_half_underaged = db.loc[\n",
    "    (\n",
    "        (db[\"0 t/m 4 jaar|2023\"] + db[\"5 t/m 14 jaar|2023\"] + db[\"15 t/m 19 jaar|2023\"])\n",
    "        / db[\"Aantal inwoners per 1-1|2023\"]\n",
    "    )\n",
    "    < 0.5,\n",
    "    :,\n",
    "]\n",
    "nm_half_underaged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06aae31",
   "metadata": {},
   "source": [
    "* Combining queries.\n",
    "\n",
    "Now all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 toddlers but less than 1,500 in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m25Toddlers_nm1500Total = db.loc[\n",
    "    (db[\"0 t/m 4 jaar|2023\"] > 25) & (db[\"Aantal inwoners per 1-1|2023\"] < 1500), :\n",
    "]\n",
    "m25Toddlers_nm1500Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5982c",
   "metadata": {},
   "source": [
    "* Sorting.\n",
    "\n",
    "Among the many operations `DataFrame` objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by Toddlers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46390189",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_Toddlers_sorted = db.sort_values(\"0 t/m 4 jaar|2023\", ascending=False)\n",
    "db_Toddlers_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5fb9c",
   "metadata": {},
   "source": [
    "If you inspect the help of `db.sort_values`, you will find that you can pass more than one column to sort the table by. This allows you to do so-called hiearchical sorting: sort first based on one column, if equal then based on another column, etc.\n",
    "\n",
    "## Visual Exploration <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "The next step to continue exploring a dataset is to get a feel for what it looks like, visually. We have already learnt how to unconver and inspect specific parts of the data, to check for particular cases we might be intersted in. Now we will see how to plot the data to get a sense of the overall distribution of values. For that, we will be using the Python library [`seaborn`](http://stanford.edu/~mwaskom/software/seaborn/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a1974",
   "metadata": {},
   "source": [
    "### Histograms <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "One of the most common graphical devices to display the distribution of values in a variable is a histogram. Values are assigned into groups of equal intervals, and the groups are plotted as bars rising as high as the number of values into the group.\n",
    "\n",
    "A histogram is easily created with the following command. In this case, let us have a look at the shape of the overall population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.displot(db[\"Aantal inwoners per 1-1|2023\"], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5353c62",
   "metadata": {},
   "source": [
    "Note we are using `sns` instead of `pd`, as the function belongs to `seaborn` instead of `pandas`.\n",
    "\n",
    "We can quickly see most of the areas contain somewhere between 1,200 and 1,700 people, approx. However, there are a few areas that have many more, even up to 3,500 people.\n",
    "\n",
    "An additional feature to visualize the density of values is called `rug`, and adds a little tick for each value on the horizontal axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42dec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.displot(db[\"Aantal inwoners per 1-1|2023\"], kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e095c",
   "metadata": {},
   "source": [
    "### Kernel Density Plots <a class=\"anchor\" id=\"section_2_2\"></a>\n",
    "\n",
    "Histograms are useful, but they are artificial in the sense that a continuous variable is made discrete by turning the values into discrete groups. An alternative is kernel density estimation (KDE), which produces an empirical density function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.kdeplot(db[\"Aantal inwoners per 1-1|2023\"], fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553cd19",
   "metadata": {},
   "source": [
    "### Line and Bar plots <a class=\"anchor\" id=\"section_2_3\"></a>\n",
    "\n",
    "Another very common way of visually displaying a variable is with a line or a bar chart. For example, if we want to generate a line plot of the (sorted) total population by area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db[\"Aantal inwoners per 1-1|2023\"].sort_values(ascending=False).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e830f",
   "metadata": {},
   "source": [
    "For a bar plot all we need to do is to change an argument of the call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db[\"Aantal inwoners per 1-1|2023\"].sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f30f0",
   "metadata": {},
   "source": [
    "Note that the large number of areas makes the horizontal axis unreadable. We can try to turn the plot around by displaying the bars horizontally (see how it's just changing `bar` for `barh`). To make it readable, let us expand the plot's height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832332af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db[\"Aantal inwoners per 1-1|2023\"].sort_values().plot(kind=\"barh\", figsize=(6, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd46964",
   "metadata": {},
   "source": [
    "## (Un)tidy data <a class=\"anchor\" id=\"section3\"></a>\n",
    "\n",
    "> *Happy families are all alike; every\n",
    "unhappy family is unhappy in its own\n",
    "way.*\n",
    "\n",
    "> Leo Tolstoy.\n",
    "\n",
    "Once you can read your data in, explore specific cases, and have a first visual approach to the entire set, the next step can be preparing it for more sophisticated analysis. Maybe you are thinking of modeling it through regression, or on creating subgroups in the dataset with particular characteristics, or maybe you simply need to present summary measures that relate to a slightly different arrangement of the data than you have been presented with.\n",
    "\n",
    "For all these cases, you first need what statistician, and general R wizard, Hadley Wickham calls *\"tidy data\"*. The general idea to \"tidy\" your data is to convert them from whatever structure they were handed in to you into one that allows convenient and standardized manipulation, and that supports directly inputting the data into what he calls \"*tidy*\" analysis tools. But, at a more practical level, what is exactly *\"tidy data\"*? In Wickham's own words:\n",
    "\n",
    "> *Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is\n",
    "messy or tidy depending on how rows, columns and tables are matched up with observations,\n",
    "variables and types.*\n",
    "\n",
    "He then goes on to list the three fundamental characteristics of *\"tidy data\"*:\n",
    "\n",
    "1. Each variable forms a column.\n",
    "1. Each observation forms a row.\n",
    "1. Each type of observational unit forms a table.\n",
    "\n",
    "If you are further interested in the concept of *\"tidy data\"*, I recommend you check out the [original paper](https://www.jstatsoft.org/v59/i10/) (open access) and the [public repository](https://github.com/hadley/tidy-data) associated with it.\n",
    "\n",
    "Let us bring in the concept of \"*tidy data*\" to our own The Hague dataset. First, remember its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52697f37",
   "metadata": {},
   "source": [
    "Thinking through *tidy* lenses, this is not a tidy dataset. It is not so for each of the three conditions:\n",
    "\n",
    "* Starting by the last one (*each type of observational unit forms a table*), this dataset actually contains not one but two observational units: the different areas of The Hague, captured by `Buurten`; *and* subgroups of an area. To *tidy* up this aspect, we can create two different tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column `Aantal inwoners per 1-1|2023` into its own as a single-column table\n",
    "db_totals = db[[\"Aantal inwoners per 1-1|2023\"]]\n",
    "db_totals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table `db_subgroups` that contains every column in `db` without `Aantal inwoners per 1-1|2023`\n",
    "db_subgroups = db.drop(\"Aantal inwoners per 1-1|2023\", axis=1)\n",
    "db_subgroups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7a6ff",
   "metadata": {},
   "source": [
    "Note we use `drop` to exclude `Aantal inwoners per 1-1|2023`, but we could also use a list with the names of all the columns to keep. Additionally, notice how, in this case, the use of `drop` (which leaves `db` untouched) is preferred to that of `del` (which permanently removes the column from `db`).\n",
    "\n",
    "At this point, the table `db_totals` is tidy: every row is an observation, every column is a variable, and there is only one observational unit in the table.\n",
    "\n",
    "The other table (`db_subgroups`), however, is not entirely tidied up yet: there is only one observational unit in the table, true; but every row is not an observation, and there are variable values as the names of columns (in other words, every column is not a variable). To obtain a fully tidy version of the table, we need to re-arrange it in a way that every row is a population subgroup in an area, and there are three variables: `Buurten`, age subgroup, and count (or frequency).\n",
    "\n",
    "Because this is actually a fairly common pattern, there is a direct way to solve it in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_subgroups = db_subgroups.stack()\n",
    "tidy_subgroups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb984b55",
   "metadata": {},
   "source": [
    "The method `stack`, well, \"stacks\" the different columns into rows. This fixes our \"tidiness\" problems but the type of object that is returning is not a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdcda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tidy_subgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145bb11",
   "metadata": {},
   "source": [
    "It is a `Series`, which really is like a `DataFrame`, but with only one column. The additional information (`Buurten` and age group) are stored in what is called an multi-index. We will skip these for now, so we would really just want to get a `DataFrame` as we know it out of the `Series`. This is also one line of code away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91379943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfold the multi-index into different, new columns\n",
    "tidy_subgroupsDF = tidy_subgroups.reset_index()\n",
    "tidy_subgroupsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a460b2",
   "metadata": {},
   "source": [
    "To which we can apply to renaming to make it look better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f069e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_subgroupsDF = tidy_subgroupsDF.rename(columns={\"level_1\": \"Subgroup\", 0: \"Freq\"})\n",
    "tidy_subgroupsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27f9d6",
   "metadata": {},
   "source": [
    "Now our table is fully tidied up!\n",
    "\n",
    "### Grouping, transforming, aggregating <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "One of the advantage of tidy datasets is they allow to perform advanced transformations in a more direct way. One of the most common ones is what is called \"group-by\" operations. Originated in the world of databases, these operations allow you to group observations in a table by one of its labels, index, or category, and apply operations on the data group by group.\n",
    "\n",
    "For example, given our tidy table with population subgroups, we might want to compute the total sum of population by each group. This task can be split into two different ones:\n",
    "\n",
    "* Group the table in each of the different subgroups.\n",
    "* Compute the sum of `Freq` for each of them.\n",
    "\n",
    "To do this in `pandas`, meet one of its workhorses, and also one of the reasons why the library has become so popular: the `groupby` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931135f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_grouped = tidy_subgroupsDF.groupby(\"Subgroup\")\n",
    "pop_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea3f18",
   "metadata": {},
   "source": [
    "The object `pop_grouped` still hasn't computed anything, it is only a convenient way of specifying the grouping. But this allows us then to perform a multitude of operations on it. For our example, the sum is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_grouped.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad520b",
   "metadata": {},
   "source": [
    "Similarly, you can also obtain a summary of each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa76b7",
   "metadata": {},
   "source": [
    "We will not get into it today as it goes beyond the basics we want to cover, but keep in mind that `groupby` allows you to not only call generic functions (like `sum` or `describe`), but also your own functions. This opens the door for virtually any kind of transformation and aggregation possible.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

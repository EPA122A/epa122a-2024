{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e498cdb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  EPA-122A *Spatial* Data Science\n",
    "\n",
    "\n",
    "## Lab 5 - Part 1: Plotting, Simple Linear Regression, K-NN Regression\n",
    "\n",
    "**TU Delft**<br>\n",
    "**Q2 2024**<br>\n",
    "**Instructors:** Giacomo Marangoni, Theodoros Chatzivasileiadis <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85270120",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Learning Goals](#learning)\n",
    "* [Review of  the  `numpy` Python library](#review)\n",
    "    * [Reshaping an array](#reshaping)\n",
    "    * [Array Slicing (a reminder...)](#aslicing)\n",
    "* [Pandas Slicing (a reminder...)](#pslicing)\n",
    "    * [Positioning with `.iloc` and `loc`](#iloc)\n",
    "    * [Pandas slicing. a short example](#slicingex)\n",
    "* [Towards matplotlib and beyond (a reminder...)](#matplotlib)\n",
    "    * [Refreshing some `matplotlib` skills ](#skills)\n",
    "    * [Figures with subplots](#subplots)\n",
    "    * [Additional matplotlib resources](#additional)\n",
    "* [Simple Linear Regression](#linear)\n",
    "    * [Linear Regression. a first practical example](#linearex)\n",
    "        * [Linear Regression: Formulae](#formulae)\n",
    "    * [Building our first linear regression model](#firstlinear)\n",
    "        * [Exercise 1: Function definition](#ex1)\n",
    "        * [Exercise 2: Visualising results](#ex2)\n",
    "* [Building a model with `statsmodels` and `sklearn`](#stats-sk)\n",
    "    * [`statsmodels` and `sklearn`: an operational difference](#opdifference)\n",
    "    * [Using the `statsmodels` package](#stats)\n",
    "    * [Using the `sklearn` package](#sk)\n",
    "        * [Additional information on the `scikit-learn` library](#addinfo)\n",
    "    * [Practice with `sklearn`: Obtaining training and testing sets from a dataframe](#sk-practice)\n",
    "* [$k$ Nearest Neighbors](#kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89a0ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.notebook_repr_html\", True)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Displays the plots for us.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388b231",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"learning\"></a>\n",
    "# Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Review `numpy` including 2-D arrays and understand array reshaping\n",
    "* Use `matplotlib` to make plots\n",
    "* Feel comfortable with simple linear regression\n",
    "* Feel comfortable with $k$ nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d1533",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"review\"></a>\n",
    "# Review of  the  `numpy` Python library\n",
    "\n",
    "Throughout the first lab (Lab 00) we learned about the `numpy` library [(documentation)](http://www.numpy.org/) and its fast array structure, called the `numpy array`.\n",
    "These two concepts (numpy library and arrays) will play a crucial role when we want to explore regression models and techniques. Thus, let us jump right back into them and introduce some new ideas along the way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create an array\n",
    "my_array = np.array([1, 4, 9, 16])\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e035a9",
   "metadata": {},
   "source": [
    "We can now try to identify the 'shape' of our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of my array: {my_array.size}, or length of my array: {len(my_array)}\")\n",
    "print(f\"Shape of my array: {my_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51553a7a",
   "metadata": {},
   "source": [
    "One important thing to notice is how the array shape is displayed:\n",
    "\n",
    "* For a 1D array, .shape returns a tuple with 1 element (n,)\n",
    "* For a 2D array, .shape returns a tuple with 2 elements (n,m)\n",
    "* For a 3D array, .shape returns a tuple with 3 elements (n,m,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc757d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"reshaping\"></a>\n",
    "## Reshaping an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd490bdb",
   "metadata": {},
   "source": [
    "As we will see later on in this lab, array reshaping can be a pretty useful tool when dealing with regression models and techniques.\n",
    "We are particularly interested in understanding how to reshape a 1D array to a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3950825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to reshape a 1D array to a 2D\n",
    "my_array.reshape(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da90270",
   "metadata": {},
   "source": [
    "Numpy arrays support the same operations as lists! Below we slice and iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"array[2:4]:\", my_array[2:4])  # A slice of the array\n",
    "\n",
    "# Iterate over the array\n",
    "for ele in my_array:\n",
    "    print(\"element:\", ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d7996",
   "metadata": {},
   "source": [
    "Also, when working with arrays it is importnat to remember that `numpy` gains a lot of its efficiency from being **strongly typed** (all elements are of the same type, such as integer or floating point). If the elements of an array are of a different type, `numpy` will force them into the same type (the longest in terms of bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b40f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = np.array([1, 2.3, \"eleni\", True])\n",
    "print(type(1), type(2.3), type(\"eleni\"), type(True))\n",
    "mixed  # all elements will become strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab246f2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"aslicing\"></a>\n",
    "## Array Slicing (a reminder...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1eab68",
   "metadata": {},
   "source": [
    "Numpy arrays can be sliced, and can be iterated over with loops.  Below is a schematic illustrating slicing two-dimensional arrays\n",
    "\n",
    " ![header](figs/2dindex_v2.png)\n",
    "\n",
    "Notice that the list slicing syntax still works!\n",
    "\n",
    "`array[2:,3]` says \"in the array, get rows 2 through the end, column 3\"\n",
    "`array[3,:]` says \"in the array, get row 3, all columns\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed50882",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pslicing\"></a>\n",
    "# Pandas Slicing (a reminder...)\n",
    "\n",
    "As we will understand later, slicing can become really useful when dealing with regression models. But, considering that a big chunk of the data we'll analyse throughout the course is collected in `pandas dataframe` structures, refreshing some `pandas` slicing concepts will help us in the next steps.\n",
    "\n",
    "But, before we can look into pandas slicing techniques it can useful to look again into the difference between `.iloc` and `.loc`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbc35e",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"iloc\"></a>\n",
    "## Positioning with `.iloc` and `loc`\n",
    "\n",
    "Look at the following data frame. It is a bad example because we have duplicate values for the index but that is legal in pandas. It's just a bad practice and we are doing it to illustrate the difference between positioning with `.iloc` and `loc`. To keep rows unique, though, internally, `pandas` has its own index which in this dataframe runs from `0` to `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdacf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"A\", \"Z\", \"A\"]\n",
    "famous = pd.DataFrame(\n",
    "    {\n",
    "        \"Elton\": [\"singer\", \"Candle in the wind\", \"male\"],\n",
    "        \"Maraie\": [\"actress\", \"Do not know\", \"female\"],\n",
    "        \"num\": np.random.randn(3),\n",
    "    },\n",
    "    index=index,\n",
    ")\n",
    "famous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ec99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing elements by label can bring up duplicates!!\n",
    "famous.loc[\"A\"]  # since we want all rows is the same as famous.loc['A',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing elements by position is unique - brings up only one row\n",
    "famous.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65bacd",
   "metadata": {},
   "source": [
    "To summarise, `.iloc` is \"position-based\" (position is unique) while `.loc` is \"label-based\" (label is not unique)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660f739",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"slicingex\"></a>\n",
    "## Pandas Slicing: a short example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cast dataframe\n",
    "cast = pd.read_csv(\"data/cast.csv\", encoding=\"utf_8\")\n",
    "cast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# briefly analysing the shape of our dataframe\n",
    "cast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57425a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rows 10 to 13 (python slicing style : exclusive of end)\n",
    "cast.iloc[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get me columns 0 to 2 but all rows - use head()\n",
    "cast.iloc[:, 0:2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get me rows 10 to 13 AND only columns 0 to 2\n",
    "cast.iloc[10:13, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806591c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE: get me rows 10 to 13 (pandas slicing style : inclusive of end)\n",
    "cast.loc[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e628378",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# give me columns 'year' and 'type' by label but only for rows 5 to 10\n",
    "cast.loc[5:10, [\"year\", \"type\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a526df2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"matplotlib\"></a>\n",
    "# Towards matplotlib and beyond (a reminder...)\n",
    "<br>\n",
    "<img style=\"float: center\" src=\"https://imgs.xkcd.com/comics/convincing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8684d",
   "metadata": {},
   "source": [
    "As we already explorede in the previous labs `matplotlib` is a very powerful `python` library for making scientific plots.\n",
    "\n",
    "We will not focus too much on the internal aspects of `matplotlib` in today's lab. There are many excellent tutorials out there for `matplotlib`.  For example,\n",
    "* [`matplotlib` homepage](https://matplotlib.org/)\n",
    "* [`matplotlib` tutorial](https://github.com/matplotlib/AnatomyOfMatplotlib)\n",
    "\n",
    "Conveying your findings convincingly is an absolutely crucial part of any analysis. Therefore, you must be able to write well and make compelling visuals.  Creating informative visuals is an involved process and we won't cover that in this lab.  However, part of creating informative data visualizations means generating *readable* figures.  If people can't read your figures or have a difficult time interpreting them, they won't understand the results of your work.  Here are some non-negotiable commandments for any plot:\n",
    "* Label $x$ and $y$ axes\n",
    "* Axes labels should be informative\n",
    "* Axes labels should be large enough to read\n",
    "* Make tick labels large enough\n",
    "* Include a legend if necessary\n",
    "* Include a title if necessary\n",
    "* Use appropriate line widths\n",
    "* Use different line styles for different lines on the plot\n",
    "* Use different markers for different lines\n",
    "\n",
    "There are other important elements, but that list should get you started on your way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccfe75",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"skills\"></a>\n",
    "## Refreshing some `matplotlib` skills\n",
    "\n",
    "To refresh our matplolib skills, we can start by generating data we'll use for plotting purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f508f63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We will use the following three functions (*don't worry about the math of these functions - they are examples*) to generate new data and create different plots:\n",
    "\n",
    "* Logistic function:\n",
    "  \\begin{align*}\n",
    "    f\\left(z\\right) = \\dfrac{1}{1 + be^{-az}}\n",
    "  \\end{align*}\n",
    "  where $a$ and $b$ are parameters.\n",
    "\n",
    "* Hyperbolic tangent:\n",
    "  \\begin{align*}\n",
    "    g\\left(z\\right) = b\\tanh\\left(az\\right) + c\n",
    "  \\end{align*}\n",
    "  where $a$, $b$, and $c$ are parameters.\n",
    "\n",
    "* Rectified Linear Unit:\n",
    "  \\begin{align*}\n",
    "    h\\left(z\\right) =\n",
    "    \\left\\{\n",
    "      \\begin{array}{lr}\n",
    "        z, \\quad z > 0 \\\\\n",
    "        \\epsilon z, \\quad z\\leq 0\n",
    "      \\end{array}\n",
    "    \\right.\n",
    "  \\end{align*}\n",
    "  where $\\epsilon < 0$ is a small, positive parameter.\n",
    "\n",
    "You are given the code for the first two functions.  Notice that $z$ is passed in as a `numpy` array and that the functions are returned as `numpy` arrays.  Parameters are passed in as floats.\n",
    "\n",
    "You should write a function to compute the rectified linear unit.  The input should be a `numpy` array for $z$ and a positive float for $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa48d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    \"\"\"Compute logistic function\n",
    "    Inputs:\n",
    "       a: exponential parameter\n",
    "       b: exponential prefactor\n",
    "       z: numpy array; domain\n",
    "    Outputs:\n",
    "       f: numpy array of floats, logistic function\n",
    "    \"\"\"\n",
    "\n",
    "    den = 1.0 + b * np.exp(-a * z)\n",
    "    return 1.0 / den\n",
    "\n",
    "\n",
    "def stretch_tanh(z: np.ndarray, a: float, b: float, c: float) -> np.ndarray:\n",
    "    \"\"\"Compute stretched hyperbolic tangent\n",
    "    Inputs:\n",
    "       a: horizontal stretch parameter (a>1 implies a horizontal squish)\n",
    "       b: vertical stretch parameter\n",
    "       c: vertical shift parameter\n",
    "       z: numpy array; domain\n",
    "    Outputs:\n",
    "       g: numpy array of floats, stretched tanh\n",
    "    \"\"\"\n",
    "    return b * np.tanh(a * z) + c\n",
    "\n",
    "\n",
    "def relu(z: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"Compute rectificed linear unit\n",
    "    Inputs:\n",
    "       eps: small positive parameter\n",
    "       z: numpy array; domain\n",
    "    Outputs:\n",
    "       h: numpy array; relu\n",
    "    \"\"\"\n",
    "    return np.fmax(z, eps * z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea9632",
   "metadata": {},
   "source": [
    "Now let's make some plots.  First, let's just warm up and plot the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 100)  # Equally spaced grid of 100 pts between -5 and 5\n",
    "\n",
    "f = logistic(x, 1.0, 1.0)  # Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb77f6",
   "metadata": {},
   "source": [
    "We can now create a first plot using the \"standard method\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f\")\n",
    "plt.title(\"Logistic Function\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba15ef5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"subplots\"></a>\n",
    "## Figures with subplots\n",
    "\n",
    "Let's start thinking about the plots as objects. We have the `figure` object which is like a matrix of smaller plots named `axes`. You can use array notation when handling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)  # Get figure and axes objects\n",
    "\n",
    "ax.plot(x, f)  # Make a plot\n",
    "\n",
    "# Create some labels\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f\")\n",
    "ax.set_title(\"Logistic Function\")\n",
    "\n",
    "# Grid\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f3258",
   "metadata": {},
   "source": [
    "Wow, it's *exactly* the same plot!  Notice, however, the use of `ax.set_xlabel()` instead of `plt.xlabel()`.  The difference is tiny, but you should be aware of it.  We will use this plotting syntax from now on.\n",
    "\n",
    "What else do we need to do to make this figure better?  Folowing our **plots commandments** we can think of the following options:\n",
    "* Make labels bigger\n",
    "* Make title bigger\n",
    "* Make line thicker\n",
    "* Make tick mark labels bigger\n",
    "* Make the grid less pronounced\n",
    "* Make figure bigger\n",
    "* Change $x$ boundaries ($x$ goes from $-5$ to $5$)\n",
    "\n",
    "Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))  # Make figure bigger\n",
    "\n",
    "# Make line plot\n",
    "ax.plot(x, f, lw=4)  # making the plot line thicker by specifying its width: lw=...\n",
    "\n",
    "# Update ticklabel size\n",
    "ax.tick_params(labelsize=24)\n",
    "\n",
    "# Creating labels\n",
    "ax.set_xlabel(\n",
    "    r\"$x$\", fontsize=24\n",
    ")  # making the x-label bigger ; Use TeX for mathematical rendering\n",
    "ax.set_ylabel(\n",
    "    r\"$f(x)$\", fontsize=24\n",
    ")  # making the y-label bigger; Use TeX for mathematical rendering\n",
    "\n",
    "# Creating title\n",
    "ax.set_title(\"Logistic Function\", fontsize=24)  # making the title bigger\n",
    "\n",
    "# Creating grid\n",
    "ax.grid(\n",
    "    True, lw=1.5, ls=\"--\", alpha=0.75\n",
    ")  # Modifying grid width: lw=... ; Modifying line style: ls=... ; Modifying grid transparency: alpha=...\n",
    "\n",
    "# Changing x boundaries\n",
    "ax.set_xlim(-5, 5, emit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad1a5a",
   "metadata": {},
   "source": [
    "Notice:\n",
    "* `lw` stands for `linewidth`.  We could also write `ax.plot(x, f, linewidth=4)`\n",
    "* `ls` stands for `linestyle`.\n",
    "* `alpha` stands for transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c078553",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"additional\"></a>\n",
    "## Additional `matplotlib` resources\n",
    "\n",
    "If you want to see all the styles available, please take a look at the documentation.\n",
    "* [Line styles](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_linestyle)\n",
    "* [Marker styles](https://matplotlib.org/stable/api/markers_api.html)\n",
    "* [Everything you could ever want](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_marker)\n",
    "\n",
    "We haven't discussed it yet, but you can also put a legend on a figure. Here are some additional resources:\n",
    "* [Legend](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html)\n",
    "* [Grid](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.grid.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43936be",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"linear\"></a>\n",
    "# Simple Linear Regression\n",
    "\n",
    "Linear regression and its many extensions are a workhorse of the statistics and data science community, both in application and as a reference point for other models. Most of the major concepts in machine learning can be and often are discussed in terms of various linear regression models. Thus, this section will introduce you to building and fitting linear regression models and some of the process behind it, so that you can:\n",
    "1) Fit models to data you encounter;\n",
    "2) Experiment with different kinds of linear regression and observe their effects;\n",
    "3) See some of the technology that makes regression models work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f987d5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"linearex\"></a>\n",
    "## Linear Regression: a first practical example\n",
    "\n",
    "We first examine a *toy problem*, focusing our efforts on fitting a linear model to a small dataset with three **observations**.  Each observation consists of one **predictor** $x_i$ and one **response**\n",
    "$y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed during lecture.\n",
    "\n",
    "To find the values of $\\beta_0$ and $\\beta_1$ we can take a closer look to the *fundamental* linear regression formulae presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our predictor and response arrays (x_train and y_train)\n",
    "x_train = np.array([1, 2, 3])\n",
    "y_train = np.array([2, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382401a6",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"formulae\"></a>\n",
    "### Linear Regression: Formulae\n",
    "\n",
    "Linear regression is special among the models we study because it can be solved explicitly. While most other models (and even some advanced versions of linear regression) must be solved itteratively, linear regression has a formula where you can simply plug in the data.\n",
    "\n",
    "For the single predictor case it is:\n",
    "    \\begin{align}\n",
    "      \\beta_1 &= \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}\\\\\n",
    "      \\beta_0 &= \\bar{y} - \\beta_1\\bar{x}\\\n",
    "    \\end{align}\n",
    "\n",
    "Where $\\bar{y}$ and $\\bar{x}$ are the mean of the y values and the mean of the x values, respectively.\n",
    "\n",
    "With this formulae in mind, let us create our first linear regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35baa94",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"firstlinear\"></a>\n",
    "##  Building our first linear regression model\n",
    "\n",
    "In this part, we will solve the equations for a simple linear regression and find the best fit solution to our *toy problem*.\n",
    "\n",
    "The snippets of code below implement the linear regression equations on the given predictors and responses, which we'll call the training data set.\n",
    "\n",
    "The code below will walk us through the following main actions:\n",
    "\n",
    "1. Reshaping ou arrays (x_train and y_train) to proper 2D dimensions.\n",
    "2. Obtaining the mean values of x_train and y_train.\n",
    "3. Calculating the numerator and denominator of the linear regression formulae.\n",
    "4. Calculating the values of  $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "**NOTE:** We'll understand later why our arrays need to be reshaped into a certain 2D format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping our arrays to be proper 2D arrays\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the mean values of x_train and y_train\n",
    "y_bar = np.mean(y_train)\n",
    "x_bar = np.mean(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating numerator and denominator of linear regression formulae\n",
    "numerator = np.sum((x_train - x_bar) * (y_train - y_bar))\n",
    "denominator = np.sum((x_train - x_bar) ** 2)\n",
    "\n",
    "print(numerator.shape, denominator.shape)  # check shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33e037",
   "metadata": {},
   "source": [
    "Why the empty brackets? The numerator and denominator are scalars, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating beta1\n",
    "beta_1 = numerator / denominator\n",
    "\n",
    "# calculating beta0\n",
    "beta_0 = y_bar - beta_1 * x_bar\n",
    "\n",
    "print(\"The best-fit line is {0:3.2f} + {1:3.2f} * x\".format(beta_0, beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1b399",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ex1\"></a>\n",
    "### Exercise 1: Function definition\n",
    "\n",
    "Turn the code from the above cells into a function called `simple_linear_regression_fit`, that inputs the training data and returns `beta0` and `beta1`.\n",
    "\n",
    "To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output.\n",
    "\n",
    "```python\n",
    "def simple_linear_regression_fit(x_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    return\n",
    "```\n",
    "\n",
    "Check your function by calling it with the training data from above and printing out the beta values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80629e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Your code here\n",
    "First try it yourself and if it dosesnt work, try again. Still doesn't work? Try working with a friend in groups. Nothing? Ok, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/simple_linear_regression_fit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31d4dd",
   "metadata": {},
   "source": [
    "Let's now run this function and see the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a605793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "betas = simple_linear_regression_fit(x_train, y_train)\n",
    "\n",
    "beta_0 = betas[0]\n",
    "beta_1 = betas[1]\n",
    "\n",
    "print(\"The best-fit line is {0:8.6f} + {1:8.6f} * x\".format(beta_0, beta_1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239f531",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ex2\"></a>\n",
    "### Exercise 2: Visualising results\n",
    "\n",
    "1. Do the values of `beta0` and `beta1` seem reasonable?\n",
    "2. Plot the training data using a scatter plot.\n",
    "3. Plot the best fit line with `beta0` and `beta1` together with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab851ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Your code here\n",
    "First try it yourself and if it doesn't work, try again. Stil doesn't work? Try working with a friend in groups. Nothing? Ok, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/best_fit_scatterplot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac343ccb",
   "metadata": {},
   "source": [
    "The values of `beta0` and `beta1` seem roughly reasonable.  They capture the positive correlation.  The line does appear to be trying to get as close as possible to all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a974e7",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stats-sk\"></a>\n",
    "# Building a regression model with `statsmodels` and `sklearn`\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two `python` packages to do it all for us:\n",
    "* [statsmodels](https://www.statsmodels.org/stable/regression.html) and\n",
    "* [scikit-learn (sklearn)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from `statsmodels` and `sklearn` to the $\\beta$ values that we found from above with our own implementation.\n",
    "\n",
    "For the purposes of this lab, `statsmodels` and `sklearn` do the same thing.  More generally though, `statsmodels` tends to be easier for inference \\[finding the values of the slope and intercept and dicussing uncertainty in those values\\], whereas `sklearn` has machine-learning algorithms and is better for prediction \\[guessing y values for a given x value\\]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for.\n",
    "\n",
    "**Note:** `statsmodels` and `sklearn` are different packages! Unless we specify otherwise, you can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ef278",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"opdifference\"></a>\n",
    "## `statsmodels` and `sklearn`: an operational difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc0ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Let's say we a data set of two obsevations with one predictor and one response variable each.\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "We would then have the following two equations if we run a simple linear regression model.\n",
    "\n",
    "$$y_1=\\beta_0 + \\beta_1*x_1$$ $$y_2=\\beta_0 + \\beta_1*x_2$$ <BR>\n",
    "\n",
    "For simplicity and calculation efficiency we want to \"absorb\" the constant $\\beta_0$ into an array with $\\beta_1$ so we have only multiplication. To do this we introduce the **constant**: ${x}^0=1$<br>\n",
    "\n",
    "$$y_1=\\beta_0*{x_1}^0 + \\beta_1*x_1$$ $$y_2=\\beta_0 * {x_2}^0 + \\beta_1*x_2$$ <BR>\n",
    "\n",
    "That becomes:\n",
    "\n",
    "$$y_1=\\beta_0*1 + \\beta_1*x_1$$ $$y_2=\\beta_0 * 1 + \\beta_1*x_2$$<bR>\n",
    "\n",
    "In matrix notation:\n",
    "\n",
    "$$\n",
    "\\left [\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\ y_2 \\\\\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left [\n",
    "\\begin{array}{cc}\n",
    "1& x_1 \\\\ 1 & x_2 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left [\n",
    "\\begin{array}{c}\n",
    "\\beta_0 \\\\ \\beta_1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "<BR><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab862",
   "metadata": {},
   "source": [
    "Keeping this in mind, the **operational difference** we wanted to highlight is that while `sklearn` adds the constant for us, `statsmodels` needs us to explicitly add it by using `sm.add_constant`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554e0df",
   "metadata": {},
   "source": [
    "**NOTE: Reshaping arrays**\n",
    "\n",
    "Now that we explored the matrix notation of our linear regression problem we can finally understand why we stressed multiple times the importance of reshaping our predictor and response arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1de6a",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stats\"></a>\n",
    "## Using the `statsmodels` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd946b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb7d95",
   "metadata": {},
   "source": [
    "Below is the code for `statsmodels`.  As we already described above, `statsmodels` does not by default include the column of ones in the $X$ matrix, so we include it manually with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X matrix by appending a column of ones (constant) to x_train\n",
    "X_train = sm.add_constant(x_train)\n",
    "\n",
    "# let's see the structure of our X matrix\n",
    "print(X_train)\n",
    "\n",
    "# build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X_train)\n",
    "\n",
    "# do the fit and save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "# print(f'The regression coefficients from statsmodels are: beta_0 = {beta0_sm} and beta_1 = {beta1_sm}')\n",
    "print(\n",
    "    f\"The regression coefficients from statsmodels are: beta_0 = {beta0_sm} and beta_1 = {beta1_sm}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c19e06",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(results_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cca35",
   "metadata": {},
   "source": [
    "Now let's turn our attention to the `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330243c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sk\"></a>\n",
    "## Using the `sklearn` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ad381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the least squares model\n",
    "toyregr = linear_model.LinearRegression()\n",
    "\n",
    "# save regression info (parameters, etc) in results_skl\n",
    "results_skl = toyregr.fit(x_train, y_train)\n",
    "\n",
    "# pull the beta parameters out from results\n",
    "beta0_skl = toyregr.intercept_[0]\n",
    "beta1_skl = toyregr.coef_[0][0]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"The regression coefficients from the sklearn package are: beta_0 = {beta0_skl} and beta_1 = {beta1_skl}\".format(\n",
    "        beta0_skl, beta1_skl\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90e417",
   "metadata": {},
   "source": [
    "We should feel pretty good about ourselves now, and we're ready to move on to a real problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76988bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d5ec3",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"addinfo\"></a>\n",
    "### Additional information on the `scikit-learn` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e778c5",
   "metadata": {},
   "source": [
    "Before diving into a \"real\" problem, let's discuss more of the details of `scikit-learn` (simply called `sklearn`).\n",
    "\n",
    "`Scikit-learn` is the main `Python` machine learning library. It consists of many **learners** which can learn models from data, as well as a lot of **utility functions** such as `train_test_split()`.\n",
    "\n",
    "In `scikit-learn`, an **estimator** is a Python object that implements the methods `fit(X, y)` and `predict(T)`\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `fit()` always takes two arguments:\n",
    "```python\n",
    "estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider two estimators in this lab: `LinearRegression` and `KNeighborsRegressor`.\n",
    "\n",
    "It is very important to understand that `Xtrain` must be in the form of a **2x2 array** with each row corresponding to one sample, and each column corresponding to the feature values for that sample.\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890419c",
   "metadata": {},
   "source": [
    "![header](figs/featurematrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc439daa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b655b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sk-practice\"></a>\n",
    "## Practice with `sklearn`: Obtaining training and testing sets from a dataframe\n",
    "\n",
    "We begin by loading up the `mtcars` dataset. This data was extracted from the 1974 Motor Trend US magazine, and comprises of fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). We will load this data to a dataframe with 32 observations on 11 (numeric) variables. Here is an explanation of the features:\n",
    "\n",
    "- `mpg` is Miles/(US) gallon\n",
    "- `cyl` is Number of cylinders,\n",
    "- `disp` is\tDisplacement (cu.in.),\n",
    "- `hp` is\tGross horsepower,\n",
    "- `drat` is\tRear axle ratio,\n",
    "- `wt` is the Weight (1000 lbs),\n",
    "- `qsec` is 1/4 mile time,\n",
    "- `vs` is Engine (0 = V-shaped, 1 = straight),\n",
    "- `am` is Transmission (0 = automatic, 1 = manual),\n",
    "- `gear` is the Number of forward gears,\n",
    "- `carb` is\tNumber of carburetors.\n",
    "\n",
    "As you will see in **Exercise 5** of Homework 05, the first step you are required to complete to create a regression model using `sklearn` on the `mtcars` dataframe is to: \"load the data `mtcars` and split them into a training set and a test set.\"\n",
    "\n",
    "While loading the dataframe and analysing its fundamnetal propoerties should be pretty easy for you at this stage, splitting the data into a training and testing set is a new step. Thus, let us complete this first step together.\n",
    "\n",
    "**NOTE:** you are still invited to repeat the process illustrated in the following code lines in your homework to check whether you understood the underlying concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17254e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading mtcars dataframe\n",
    "dfcars = pd.read_csv(\"data/mtcars.csv\")\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the first column title\n",
    "dfcars = dfcars.rename(columns={\"Unnamed: 0\": \"car name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the dimensions of the dataframe\n",
    "dfcars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80291357",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9974b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a34646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set is around 20% of the total data; training set is around 80%\n",
    "print(\"Shape of full dataset is: {0}\".format(dfcars.shape))\n",
    "print(\"Shape of training dataset is: {0}\".format(traindf.shape))\n",
    "print(\"Shape of test dataset is: {0}\".format(testdf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c81513",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "To get to part 6 below, you will have to first complete the homework exercise 5.1 and then use that to complete k-nearest neighbours exercise below. The variables X_train and y_train come from the homework and it is your task to do it. If you are not able to, then solutions will be provided to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0655cd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kNN\"></a>\n",
    "# $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c00f9d",
   "metadata": {},
   "source": [
    "Now that you're familiar with `sklearn`, you're ready to do a KNN regression.\n",
    "\n",
    "Sklearn's regressor is called `sklearn.neighbors.KNeighborsRegressor`. Its main parameter is the **number of nearest neighbors**. There are other parameters such as the distance metric (default for 2 order is the Euclidean distance). For a list of all the parameters see the [Sklearn kNN Regressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html).\n",
    "\n",
    "For now, let's use $3$ nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a249f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of neighbors\n",
    "k = 3\n",
    "knnreg = KNeighborsRegressor(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da11560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regressor - make sure your numpy arrays are the right shape!\n",
    "knnreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the outcome on the train set using R^2\n",
    "r2_train = knnreg.score(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(f\"kNN model with {k} neighbors gives R^2 on the train set: {r2_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda6c34",
   "metadata": {},
   "source": [
    "We can now analyse the predicted values that our regression model originates from the given test data (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404e7a0",
   "metadata": {},
   "source": [
    "From these estimates our regression model can calculate $R^{2}$ score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bead6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_test = knnreg.score(X_test, y_test)\n",
    "print(f\"kNN model with {k} neighbors gives R^2 on the test set: {r2_test:.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092326e",
   "metadata": {},
   "source": [
    "Not so good? Lets vary the number of neighbors and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our lives easy by storing the different regressors in a dictionary\n",
    "regdict = {}\n",
    "\n",
    "# Make our lives easier by entering the k values from a list\n",
    "k_list = [1, 2, 4, 15]\n",
    "\n",
    "# Do a bunch of KNN regressions\n",
    "for k in k_list:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knnreg.fit(X_train, y_train)\n",
    "    # Store the regressors in a dictionary\n",
    "    regdict[k] = knnreg\n",
    "\n",
    "# Print the dictionary to see what we have\n",
    "regdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb3efe",
   "metadata": {},
   "source": [
    "Now let's plot all the k values in same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6100dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, \"o\", label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "\n",
    "# let's unpack the dictionary to its elements (items) which is the k and Regressor\n",
    "for k, regressor in regdict.items():\n",
    "    predictions = regressor.predict(xgrid.reshape(-1, 1))\n",
    "    ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84e410",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "Notice how the $1$-NN goes through every point on the training set but utterly fails elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a7b2a",
   "metadata": {},
   "source": [
    "Lets look at the scores on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 15)  # Grid of k's\n",
    "scores_train = []  # R2 scores\n",
    "for k in ks:\n",
    "    # Create KNN model\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "    # Fit the model to training data\n",
    "    knnreg.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate R^2 score\n",
    "    score_train = knnreg.score(X_train, y_train)\n",
    "    scores_train.append(score_train)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(ks, scores_train, \"o-\")\n",
    "ax.set_xlabel(r\"$k$\")\n",
    "ax.set_ylabel(r\"$R^{2}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf444f62",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Why do we get a perfect $R^2$ at k=1 for the training set?\n",
    "* Make the same plot as above on the *test* set.\n",
    "* What is the best $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ba89d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Your code here\n",
    "First try it yourself and if it dosesnt work, try again. Stil doesn't work? Try working with a friend in groups. Nothing? Ok, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/knn_regression.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
